#!/usr/bin/env python3
"""
finite_series

Computes E_V(r) for a 1D system through the
formula (62) in Torquato, Reformulation..., PRE 82 (2010).

Usage: finite_series [options] infiles... > outfile

infiles use Ge's text file format, and each
file contains one configuration

outfiles use format
r E_V E_V_unc
where E_V_unc is computed as std err of mean
with each configuration being treated as independent

Options:
    -h, --help: Print this message
    -o: Give the order to use in the series (62), if -1, use
        highest order possible [Default = -1]
    -c: Give the upper cutoff for computing E_V(r) [default = 1.0]
    -n: Give the number of grid points [default = 10]
"""

import sys
import numpy as np
from scipy.special import comb

# Returns a tuple (r, n), where
# r is the shortest periodic cell distance
# and n is the number of particles in between
# the two particles using that shortest distance
def dist(p_i, p_j, box_length, n_particles):
    raw_rij = np.abs(p_i[0] - p_j[0])
    raw_idx_diff = abs(p_i[1] - p_j[1])
    alt_rij = box_length - raw_rij
    if raw_rij <= alt_rij:
        return (raw_rij, raw_idx_diff - 1)
    else:
        return (alt_rij, n_particles - raw_idx_diff - 1)

# All n-point intersections can be given in terms of
# v1(R) and v2(R) in 1D
def v1(R):
    return 2.0*R

def v2(R, rij):
    return v1(R)*np.heaviside(2.0*R - rij, 0.0)*(1.0 - rij/(2.0*R))

# Returns a list of pairs, [(r, n), ...],
# where r is the distance between the pair,
# and n is the number of points between
def find_all_pairs(points, box_length, n_particles):
    n_cells = len(points)
    pairs = []

    for i in range(n_cells):
        curr_cell = points[i]
        n_in_cell = len(curr_cell)
        next_cell = points[(i+1)%n_cells]
        for j in range(n_in_cell):
            p = curr_cell[j]
            for k in range(j):
                q = curr_cell[k]
                pairs += [dist(p, q, box_length, n_particles)]
            for q in next_cell:
                pairs += [dist(p, q, box_length, n_particles)]
    
    return pairs

def resolve_pair_combinatorics(grid, pair, box_length, order, max_obs_order):
    # Rename for convenience
    rij = pair[0]
    n_between = pair[1]

    # Compute contribution to the maximum observed order needed at
    # each r
    obs_order = np.heaviside(2.0*grid - rij, 0.0)*(n_between + 2)
    temp_max_obs_order = np.fmax(max_obs_order, obs_order)
    # https://numpy.org/doc/stable/reference/generated/numpy.put.html
    max_obs_order[:] = temp_max_obs_order

    pair_intersection = v2(grid, rij)
    result = np.zeros(grid.shape)
    
    for i in range(0, n_between+1):
        if order == -1 or i + 2 <= order:
            # https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.comb.html
            # https://stackoverflow.com/questions/4941753/is-there-a-math-ncr-function-in-python
            result += np.power(-1.0, i)*comb(n_between, i)*pair_intersection

    return result/box_length

def single_configuration(grid, points, box_length, order, rho, n_particles, max_obs_order):
    result = np.ones(grid.shape) - rho*v1(grid) # First order E_V
    pairs = find_all_pairs(points, box_length, n_particles)
    
    for p in pairs:
        result += resolve_pair_combinatorics(grid, p, box_length, order, max_obs_order)
    
    return result

# Returns a cell_list, in form
#    cell_0                       cell_1
# [[(r0, idx0), (r1, idx1), ... ], [""], ...]
# This allows you to keep track of both spatial location,
# and figure out how many particles are between two particles
def make_cell_list(points, box_length, cutoff):
    # Make sure everthing is strictly in [0.0, box_length)
    for p in points:
        if p < 0.0 or p >= box_length:
            print("Particle not in box, check generating program!", file=sys.stderr)
            sys.exit(-1)

    cell_list = []
    
    # Determine the number of cells based on the cutoff
    cell_spec = np.linspace(0.0, box_length, int(box_length/(2.0*cutoff)) + 1)
    print("Number of cells: {}".format(len(cell_spec) - 1), file=sys.stderr)
    
    # Make the cell list
    for i in range(len(cell_spec) - 1):
        cell = []
        for j, p in enumerate(points):
            if p >= cell_spec[i] and p < cell_spec[i+1]:
                cell += [(p, j)]
        cell_list += [cell]
    
    return cell_list

if __name__ == "__main__":
    # Parse command line
    order = -1
    cutoff = 1.0
    n_grid = 10
    if "-h" in sys.argv or "--help" in sys.argv:
        print(__doc__)
        sys.exit()
    if "-o" in sys.argv:
        idx = sys.argv.index("-o")
        sys.argv.pop(idx)
        order = int(sys.argv.pop(idx))
    if "-c" in sys.argv:
        idx = sys.argv.index("-c")
        sys.argv.pop(idx)
        cutoff = float(sys.argv.pop(idx))
    if "-n" in sys.argv:
        idx = sys.argv.index("-n")
        sys.argv.pop(idx)
        n_grid = int(sys.argv.pop(idx))
    infiles = [open(fname) for fname in sys.argv[1:]]
    n_files = len(infiles)

    # Determine evaluation grid
    grid = np.linspace(0.0, cutoff, num=n_grid)
    e_v = np.zeros(grid.shape)
    e_v_unc = np.zeros(grid.shape)
    # The highest number of intersections needed to compute each r
    max_obs_order = np.zeros(grid.shape)

    # Compute E_V(r) over grid for each configuration
    for n, f in enumerate(infiles):
        if n%100 == 0:
            print("Working on file {}".format(n), file=sys.stderr)
        # Read file info
        f.readline()
        box_length = float(f.readline().split()[0])
        raw_points = np.loadtxt(f)[:,0]
        rho = float(len(raw_points))/box_length
        
        # Sort point list
        raw_points.sort()
        # Compute cell_list
        points = make_cell_list(raw_points, box_length, cutoff)

        # Compute for a single configuration
        temp_e_v = single_configuration(grid,
                                        points,
                                        box_length,
                                        order,
                                        rho,
                                        len(raw_points),
                                        max_obs_order
                                        )
        e_v += temp_e_v
        e_v_unc += temp_e_v*temp_e_v

    # Postprocess mean and uncertainty
    e_v /= n_files
    e_v_unc = np.sqrt((e_v_unc - n_files*e_v*e_v)/(n_files - 1))
    e_v_unc /= np.sqrt(float(n_files))

    # Output results
    np.savetxt(sys.stdout.buffer,
            np.stack([
                grid,
                e_v,
                e_v_unc,
                max_obs_order], -1)
            )
