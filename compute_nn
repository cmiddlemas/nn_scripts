#!/usr/bin/env python3
"""
compute_nn
Author: Timothy Middlemas

Program to sample all of H_V(r), E_V(r), G_V(r) and
the corresponding particle quantities at once

Uses Scipy's cKDTree implementation for lookup

Implements periodic boundary conditions by 1 layer of
replication. Note that this is usually, but not always
correct, so be careful.

Usage: compute_nn [infiles ...] > [outfile]

Will interpret multiple infiles as ensemble average

Outputs the following table on stdout, space separated
bin_center bin_size H_V unc_H_V E_V unc_E_V G_V unc_G_V

Uncertainties given as std err of mean, with each binned configuration
serving as an ''independent'' data point

Options:
    -h, --help -> Print this message
    -n [bins] -> number of bins (default: 50)
    -c [cutoff] -> cutoff for sampling (default: 1.0)
    -s [samples] -> number of sample points per configuration for void
                    quantities (default: 1000)
    --cycles [cycles] -> number of times to repeat samples on a single
                         configuration, use if -s [samples] would
                         create arrays too large for your ram.
                         do not use for particle quanties, will
                         lead to wrong error bars
                         (default: 1)
    -p -> Compute particle quantities instead of void
"""

import sys
import numpy as np
from scipy.spatial import cKDTree

# Volume of the fundamental cell passed as a matrix
def basis_vol(basis):
    return np.linalg.det(basis)

# Volume of sphere of radius r in dimension dim
def sphere_vol(dim, r):
    if dim == 1:
        return 2*r
    elif dim == 2:
        return np.pi*r*r
    elif dim == 3:
        return 4.0*np.pi*r*r*r/3.0
    else:
        print("Dimension not implemented", file=sys.stderr)
        sys.exit(-1)

if __name__ == "__main__":
    # Parse command line
    n_bins = 50
    cutoff = 1.0
    n_samples = 1000
    n_cycles = 1
    compute_particle = False

    if "--help" in sys.argv or "-h" in sys.argv:
        print(__doc__)
        sys.exit()
    # https://stackoverflow.com/questions/9542738/python-find-in-list
    if "-n" in sys.argv:
        idx = sys.argv.index("-n")
        sys.argv.pop(idx)
        n_bins = int(sys.argv.pop(idx))
    if "-c" in sys.argv:
        idx = sys.argv.index("-c")
        sys.argv.pop(idx)
        cutoff = float(sys.argv.pop(idx))
    if "-s" in sys.argv:
        idx = sys.argv.index("-s")
        sys.argv.pop(idx)
        n_samples = int(sys.argv.pop(idx))
    if "--cycles" in sys.argv:
        idx = sys.argv.index("--cycles")
        sys.argv.pop(idx)
        n_cycles = int(sys.argv.pop(idx))
    if "-p" in sys.argv:
        sys.argv.remove("-p")
        compute_particle = True
    
    n_ensemble = len(sys.argv) - 1 # number of configurations in ensemble
    input_paths = sys.argv[1:]

    # Compute bin locations and widths
    bin_size = cutoff/float(n_bins)
    bin_boundaries = np.linspace(0.0, cutoff, num=n_bins+1)
    right_bin_boundaries = bin_boundaries[1:]
    left_bin_boundaries = bin_boundaries[:-1]
    bin_centers = right_bin_boundaries - bin_size/2.0

    # Initialize accumulators
    h = np.zeros(n_bins) # Nearest neighbor function
    h_unc = np.zeros(n_bins)
    e = np.zeros(n_bins) # Complementary cumulative function
    e_unc = np.zeros(n_bins)
    g_ratio = np.zeros(n_bins)
    g_ratio_unc = np.zeros(n_bins)
    density = 0.0

    dim = 0
    for m, path in enumerate(input_paths): # Loop over configurations
        if m%100 == 0:
            # https://stackoverflow.com/questions/5574702/how-to-print-to-stderr-in-python
            print("Working on file " + str(m), file=sys.stderr)
        # Read necessary stuff from file
        infile = open(path)
        dim = int(infile.readline())
        basis = np.loadtxt(infile, max_rows=dim)[:, :dim]
        # https://stackoverflow.com/questions/31698242/python-how-can-i-force-1-element-numpy-arrays-to-be-two-dimensional
        points = np.atleast_2d(np.loadtxt(infile))[:, :dim]
        n_points = len(points)
        density += float(n_points)/basis_vol(basis)

        # Construct periodic images
        periodic = np.tile(points, (3**dim, 1))
        for i in range(3**dim):
            for j in range(dim):
                a = i
                for k in range(j):
                    a = a//3
                a = (a%3) - 1
                periodic[i*n_points:(i+1)*n_points, :] += a*basis[j, :]
        
        # Handles efficient nearest-neighbor lookup
        # https://stackoverflow.com/questions/31819778/scipy-spatial-ckdtree-running-slowly
        tree = cKDTree(periodic, balanced_tree=False, compact_nodes=False)

        h_count = np.zeros(n_bins)
        e_count = np.zeros(n_bins)
        # Possibly repeat sampling due to memory limitations
        for _ in range(n_cycles):
            if compute_particle:
                samples = points
                n_samples = n_points

                # Generate nn distances, k=2 because we
                # always get the point itself as first match
                dd, ii = tree.query(samples, k=2, n_jobs=-1,
                        distance_upper_bound=cutoff)
                nn_dist = dd[:,1]
            else: # compute the void nearest neighbor functions
                # Choose spatial points to sample randomly
                samples = np.random.rand(n_samples, dim) @ basis

                # Generate nn distances
                dd, ii = tree.query(samples, k=1, n_jobs=-1,
                        distance_upper_bound=cutoff)
                nn_dist = dd
            
            # Bin samples
            tiled_right = np.tile(right_bin_boundaries, (n_samples, 1))
            tiled_left = np.tile(left_bin_boundaries, (n_samples, 1))
            tiled_center = np.tile(bin_centers, (n_samples, 1))
            # https://stackoverflow.com/questions/17428621/python-differentiating-between-row-and-column-vectors
            bin_lt_right = np.less(nn_dist[:, np.newaxis], tiled_right, dtype=float)
            bin_gte_left = np.greater_equal(nn_dist[:, np.newaxis], tiled_left, dtype=float)
            bin_gte_center = np.greater_equal(nn_dist[:, np.newaxis], tiled_center, dtype=float)
                
            # Use bin values to update accumulators
            # For each sample, add nn to correct bin
            h_count += np.sum(bin_lt_right*bin_gte_left, axis=0)
            # For each sample, keep track of counts greater than each bin center
            e_count += np.sum(bin_gte_center, axis=0)
        
        # Update h, e, and uncertainties with count from all the cycles,
        # assuming each configuration gives an independent data point
        h += h_count
        h_unc += h_count*h_count
        e += e_count
        e_unc += e_count*e_count
        g_ratio_count = h_count/e_count
        g_ratio += g_ratio_count
        g_ratio_unc += g_ratio_count*g_ratio_count

    # Use the full number of samples for normalization
    n_samples = n_samples*n_cycles
    # Normalize h, e and density
    h /= float(n_samples*n_ensemble)*bin_size 
    e /= float(n_samples*n_ensemble)
    density /= float(n_ensemble)
    # Normalize g_ratio
    rho_s1_dr = density*(sphere_vol(dim, right_bin_boundaries) - sphere_vol(dim,
        left_bin_boundaries))
    g_ratio /= rho_s1_dr*float(n_ensemble)

    # Compute uncertainties in h and e, sample std err
    h_unc /= (float(n_samples)*bin_size) ** 2.0
    h_unc = (h_unc - float(n_ensemble)*h*h)/float(n_ensemble - 1)
    h_unc = np.sqrt(h_unc/float(n_ensemble))
    e_unc /= float(n_samples) ** 2.0
    e_unc = (e_unc - float(n_ensemble)*e*e)/float(n_ensemble - 1)
    e_unc = np.sqrt(e_unc/float(n_ensemble))

    # Compute uncertainty in g_ratio
    g_ratio_unc /= rho_s1_dr ** 2.0
    g_ratio_unc = (g_ratio_unc -
            float(n_ensemble)*g_ratio*g_ratio)/float(n_ensemble-1)
    g_ratio_unc = np.sqrt(g_ratio_unc/float(n_ensemble))
    
    # Compute g function from ensemble averaged e and h
    g = h*bin_size
    g /= e 
    g /= rho_s1_dr
    
    # Compute uncertainty in g, assuming h and e are independent (not actually
    # Not using because it allows for error bars extending past zero,
    # and looks qualitatively similar to below method elsewhere
    # g_unc = bin_size * np.sqrt( h_unc*h_unc/(e*e) + h*h*e_unc*e_unc/(e*e*e*e) ) / rho_s1_dr

    # Compute uncertainty in g, assuming all uncertainty comes from h
    # I think this works because we can think of the computation
    # as computed a binning uncertainty over a subset of the total
    # data, could test if worried about this reasoning by
    # implementing full covariance estimation for h and e
    g_unc = bin_size * h_unc / (e * rho_s1_dr)

    # Save output to stdout
    np.savetxt(sys.stdout.buffer,
            np.stack([bin_centers, np.full(n_bins,bin_size), h, h_unc, e, e_unc,
                g, g_unc, g_ratio, g_ratio_unc], -1)
            )
