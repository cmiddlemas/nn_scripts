#!/usr/bin/env python3
"""
compute_nn
Author: Timothy Middlemas

Program to sample all of H_V(r), E_V(r), G_V(r) and
the corresponding particle quantities at once

Uses Scipy's cKDTree implementation for lookup

Implements periodic boundary conditions by 1 layer of
replication. Note that this is usually, but not always
correct, so be careful.

Usage: compute_nn [infiles ...] > [outfile]

Will interpret multiple infiles as ensemble average

Outputs a table on stdout, as space separated columns
The meaning of each column is as follows
1 bin_center
2 bin_size
3 count_H_V
4 count_E_V
5 H_V
6 unc_H_V
7 E_V
8 unc_E_V
9 G_V
10 unc_G_V
11 var_G_V
12 unc2_G_V
13 ratio_G_V
14 unc_ratio_G_V

Most of these are self explanator. count_E_V gives
the raw counts for E_V determination, so that one can easily
make a rough cutoff based on requiring a small number of samples
to have good statistics

Uncertainties for H_V and E_V given as std err of mean, with each binned
configuration serving as an ''independent'' data point

Uncertainty for G_V is little more complicated. There
are two ways of computing G_V, one whic is <H>/<E> and
one which is <H/E>. The "normal" G_V is the former,
while the "ratio" G_V is the latter.

We estimate the variance in G_V through a non-linear
error propagation using the H-E covariance matrix.
Sometimes this method is bad, since H and E can become
highly correlated in the tail, leading to 0s (and nans if
there is floating point error). Thus, we report, in addition
to the uncertainty, the variance (i.e. before the square root)
and the uncertainty estimated by assuming the value of E is perfect
(unc2_G_V) for comparison. This lets one be confident that any nans that
arise are due simply to floating point error.

Estimating the uncertainty in unc_ratio_G_V is done by
the std err of the mean, but it is worth noting that
we only compute ratio_G_V for those bins where for
every configuration, G_V is not nan.

Options:
    -h, --help -> Print this message
    -n [bins] -> number of bins (default: 50)
    -c [cutoff] -> cutoff for sampling (default: 1.0)
    -s [samples] -> number of sample points per configuration for void
                    quantities (default: 1000)
    --cycles [cycles] -> number of times to repeat samples on a single
                         configuration, use if -s [samples] would
                         create arrays too large for your ram.
                         do not use for particle quanties, will
                         lead to wrong error bars
                         (default: 1)
    -b [block] [block_file] -> Also do a post-division
                               averaging of G_V(r), combining
                               [block] data points into one
                               and output this analysis into
                               the [block_file]. uses same
                               format as stdout output, but only prints
                               columns 1, 9, and 10, in that order.
                               Propagates uncertainty
                               by standard addition formula, assuming independence
                               of each spatial point.
    -p -> Compute particle quantities instead of void
    -d [drop] [drop_file] -> drop bins with hcount < [drop], print results of this
                             analysis to [drop_file], if used with -b, will use
                             post-drop g to do post-averaging. Prints columns
                             1, 5, 6, 7, 8, 9, 10, in that order.
"""

import sys
import numpy as np
from scipy.spatial import cKDTree

# Volume of the fundamental cell passed as a matrix
def basis_vol(basis):
    return np.linalg.det(basis)

# Volume of sphere of radius r in dimension dim
def sphere_vol(dim, r):
    if dim == 1:
        return 2*r
    elif dim == 2:
        return np.pi*r*r
    elif dim == 3:
        return 4.0*np.pi*r*r*r/3.0
    else:
        print("Dimension not implemented", file=sys.stderr)
        sys.exit(-1)

if __name__ == "__main__":
    # Parse command line
    n_bins = 50
    cutoff = 1.0
    n_samples = 1000
    n_cycles = 1
    use_blocks = False
    compute_particle = False
    drop_bins = False

    if "--help" in sys.argv or "-h" in sys.argv:
        print(__doc__)
        sys.exit()
    # https://stackoverflow.com/questions/9542738/python-find-in-list
    if "-n" in sys.argv:
        idx = sys.argv.index("-n")
        sys.argv.pop(idx)
        n_bins = int(sys.argv.pop(idx))
    if "-c" in sys.argv:
        idx = sys.argv.index("-c")
        sys.argv.pop(idx)
        cutoff = float(sys.argv.pop(idx))
    if "-s" in sys.argv:
        idx = sys.argv.index("-s")
        sys.argv.pop(idx)
        n_samples = int(sys.argv.pop(idx))
    if "--cycles" in sys.argv:
        idx = sys.argv.index("--cycles")
        sys.argv.pop(idx)
        n_cycles = int(sys.argv.pop(idx))
    if "-b" in sys.argv:
        use_blocks = True
        idx = sys.argv.index("-b")
        sys.argv.pop(idx)
        block_size = int(sys.argv.pop(idx))
        block_file = open(sys.argv.pop(idx), mode='w')
    if "-p" in sys.argv:
        sys.argv.remove("-p")
        compute_particle = True
    if "-d" in sys.argv:
        drop_bins = True
        idx = sys.argv.index("-d")
        sys.argv.pop(idx)
        drop_cutoff = int(sys.argv.pop(idx))
        drop_file = open(sys.argv.pop(idx), mode='w')

    
    n_ensemble = len(sys.argv) - 1 # number of configurations in ensemble
    input_paths = sys.argv[1:]

    # Compute bin locations and widths
    bin_size = cutoff/float(n_bins)
    bin_boundaries = np.linspace(0.0, cutoff, num=n_bins+1)
    right_bin_boundaries = bin_boundaries[1:]
    left_bin_boundaries = bin_boundaries[:-1]
    bin_centers = right_bin_boundaries - bin_size/2.0

    # Initialize accumulators
    h = np.zeros(n_bins) # Nearest neighbor function
    h_unc = np.zeros(n_bins)
    e = np.zeros(n_bins) # Complementary cumulative function
    e_unc = np.zeros(n_bins)
    he_cov = np.zeros(n_bins)
    g_ratio = np.zeros(n_bins)
    g_ratio_unc = np.zeros(n_bins)
    density = 0.0

    dim = 0
    for m, path in enumerate(input_paths): # Loop over configurations
        if m%100 == 0:
            # https://stackoverflow.com/questions/5574702/how-to-print-to-stderr-in-python
            print("Working on file " + str(m), file=sys.stderr)
        # Read necessary stuff from file
        infile = open(path)
        dim = int(infile.readline())
        basis = np.atleast_2d(np.loadtxt(infile, max_rows=dim))[:, :dim]
        # https://stackoverflow.com/questions/31698242/python-how-can-i-force-1-element-numpy-arrays-to-be-two-dimensional
        points = np.atleast_2d(np.loadtxt(infile))[:, :dim]
        n_points = len(points)
        density += float(n_points)/basis_vol(basis)

        # Construct periodic images
        periodic = np.tile(points, (3**dim, 1))
        for i in range(3**dim):
            for j in range(dim):
                a = i
                for k in range(j):
                    a = a//3
                a = (a%3) - 1
                periodic[i*n_points:(i+1)*n_points, :] += a*basis[j, :]
        
        # Handles efficient nearest-neighbor lookup
        # https://stackoverflow.com/questions/31819778/scipy-spatial-ckdtree-running-slowly
        tree = cKDTree(periodic, balanced_tree=False, compact_nodes=False)

        h_count = np.zeros(n_bins)
        e_count = np.zeros(n_bins)
        # Uncomment to also compute e based on left boundary of each bin
        # e_left_count = np.zeros(n_bins)
        # Possibly repeat sampling due to memory limitations
        for _ in range(n_cycles):
            if compute_particle:
                samples = points
                n_samples = n_points

                # Generate nn distances, k=2 because we
                # always get the point itself as first match
                dd, ii = tree.query(samples, k=2, n_jobs=-1,
                        distance_upper_bound=cutoff)
                nn_dist = dd[:,1]
            else: # compute the void nearest neighbor functions
                # Choose spatial points to sample randomly
                samples = np.random.rand(n_samples, dim) @ basis

                # Generate nn distances
                dd, ii = tree.query(samples, k=1, n_jobs=-1,
                        distance_upper_bound=cutoff)
                nn_dist = dd
            
            # Bin samples
            tiled_right = np.tile(right_bin_boundaries, (n_samples, 1))
            tiled_left = np.tile(left_bin_boundaries, (n_samples, 1))
            tiled_center = np.tile(bin_centers, (n_samples, 1))
            # https://stackoverflow.com/questions/17428621/python-differentiating-between-row-and-column-vectors
            bin_lt_right = np.less(nn_dist[:, np.newaxis], tiled_right, dtype=float)
            bin_gte_left = np.greater_equal(nn_dist[:, np.newaxis], tiled_left, dtype=float)
            bin_gte_center = np.greater_equal(nn_dist[:, np.newaxis], tiled_center, dtype=float)
                
            # Use bin values to update accumulators
            # For each sample, add nn to correct bin
            h_count += np.sum(bin_lt_right*bin_gte_left, axis=0)
            # For each sample, keep track of counts greater than each bin center
            e_count += np.sum(bin_gte_center, axis=0)
            # e_left_count += np.sum(bin_gte_left, axis=0)
        
        # Update h, e, and uncertainties with count from all the cycles,
        # assuming each configuration gives an independent data point
        h += h_count
        h_unc += h_count*h_count
        e += e_count
        e_unc += e_count*e_count
        he_cov += h_count*e_count
        # Use center of bin
        g_ratio_count = h_count/e_count
        # Use lower limit of bin
        # g_ratio_count = h_count/e_left_count
        g_ratio += g_ratio_count
        g_ratio_unc += g_ratio_count*g_ratio_count

    # Save the raw final counts for interpretation purposes
    h_final_count = np.copy(h)
    e_final_count = np.copy(e)
    # Use the full number of samples for normalization
    n_samples = n_samples*n_cycles
    # Normalize h, e and density
    h /= float(n_samples*n_ensemble)*bin_size 
    e /= float(n_samples*n_ensemble)
    density /= float(n_ensemble)
    # Normalize g_ratio
    rho_s1_dr = density*(sphere_vol(dim, right_bin_boundaries) - sphere_vol(dim,
        left_bin_boundaries))
    g_ratio /= rho_s1_dr*float(n_ensemble)

    # Compute uncertainties in h and e, sample std err
    h_unc /= (float(n_samples)*bin_size) ** 2.0
    h_unc = (h_unc - float(n_ensemble)*h*h)/float(n_ensemble - 1)
    h_unc = np.sqrt(h_unc/float(n_ensemble))
    e_unc /= float(n_samples) ** 2.0
    e_unc = (e_unc - float(n_ensemble)*e*e)/float(n_ensemble - 1)
    e_unc = np.sqrt(e_unc/float(n_ensemble))
    # Compute covariance of h and e, with mean correction
    # https://stats.stackexchange.com/questions/163583/covariance-of-two-sample-means
    # https://stats.stackexchange.com/questions/142456/why-shouldnt-the-denominator-of-the-covariance-estimator-be-n-2-rather-than-n-1/142472#142472
    he_cov /= bin_size*(float(n_samples) ** 2.0)
    he_cov = (he_cov - float(n_ensemble)*h*e)/float(n_ensemble - 1)
    he_cov /= float(n_ensemble)

    # Compute uncertainty in g_ratio
    g_ratio_unc /= rho_s1_dr ** 2.0
    g_ratio_unc = (g_ratio_unc -
            float(n_ensemble)*g_ratio*g_ratio)/float(n_ensemble-1)
    g_ratio_unc = np.sqrt(g_ratio_unc/float(n_ensemble))
    
    # Compute g function from ensemble averaged e and h
    g = h*bin_size
    g /= e 
    g /= rho_s1_dr
    
    # Compute uncertainty in g, assuming all uncertainty comes from h
    # This is a backup estimator, since sometimes the full covariance
    # estimate becomes degenerate in the tail
    g_unc_ind_h = bin_size * h_unc / (e * rho_s1_dr)
    
    # Compute uncertainty in g through full covariance analysis
    # Print out variance as well, just in case a cancellation happens
    # and we get a nan, we can verify that the most likely cause is
    # simply floating point error on the cancellation
    g_var = ((bin_size / rho_s1_dr) ** 2.0) * (
                h_unc*h_unc/(e*e) + h*h*e_unc*e_unc/(e*e*e*e) - 2*h*he_cov/(e*e*e)
                )
    g_unc = np.sqrt(g_var)

    # Save output to stdout
    np.savetxt(sys.stdout.buffer,
            np.stack([
                bin_centers,
                np.full(n_bins, bin_size),
                h_final_count,
                e_final_count,
                h,
                h_unc,
                e,
                e_unc,
                g,
                g_unc,
                g_var,
                g_unc_ind_h,
                g_ratio,
                g_ratio_unc], -1)
            )

    # drop bins with hcount < drop_cutoff, save
    # result to drop_file
    # https://stackoverflow.com/questions/38193958/how-to-properly-mask-a-numpy-2d-array
    # and Numpy docs
    if drop_bins:
        h_mask = h_final_count >= drop_cutoff
        bin_centers = bin_centers[h_mask]
        h = h[h_mask]
        h_unc = h_unc[h_mask]
        e = e[h_mask]
        e_unc = e_unc[h_mask]
        g = g[h_mask]
        g_var = g_var[h_mask]
        g_unc = g_unc[h_mask]
        np.savetxt(drop_file,
                np.stack([
                    bin_centers,
                    h,
                    h_unc,
                    e,
                    e_unc,
                    g,
                    g_unc
                    ], -1)
                )

    # Do a post-division averaging of g
    if use_blocks:
        n_blocks = len(g)//block_size
        domain_blocks = np.zeros(n_blocks)
        g_blocks = np.zeros(n_blocks)
        g_unc_blocks = np.zeros(n_blocks)

        for i in range(n_blocks):
            g_blocks[i] = np.sum(g[block_size*i:block_size*(i+1)])
            domain_blocks[i] = np.sum(bin_centers[block_size*i:block_size*(i+1)])
            g_unc_blocks[i] = np.sqrt(np.sum(g_var[block_size*i:block_size*(i+1)]))

        # Normalize blocks
        g_blocks /= float(block_size)
        domain_blocks /= float(block_size)
        g_unc_blocks /= float(block_size)

        np.savetxt(block_file,
                np.stack([
                    domain_blocks,
                    g_blocks,
                    g_unc_blocks
                    ], -1)
                )
